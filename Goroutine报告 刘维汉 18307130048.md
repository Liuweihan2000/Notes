刘维汉 18307130048

目录：

[TOC]



### 从Go语言本身说起

Go语言诞生于2009年。这门语言的许多特性使得其开发网络应用非常迅速，并且其内置的Goroutine，即Go语言协程使得这门语言天生就能够应对超大的并发量。可以说Go语言就是为了互联网环境而生的。尽管如此，如果仔细研究其设计理念和实现原理，便会发现这里面的很多细节又和操作系统相关的概念息息相关，毕竟开发一门新的语言是无法避免和操作系统打交道的

------

### Goroutine与线程

通常来讲，内核级线程，即由操作系统创建并管理的线程，其栈的大小为1MB。这意味着如果一台互联网服务器为每一个请求都开创一个内核级线程的话，仅仅创建1000个线程便会消耗1GB的内存资源。这对于当今互联网环境下动辄几万几十万的qps来讲是远远不够的。

与内核级线程相对的一个概念就是用户级线程，即由用户进程创建的，处于这个进程内部并由这个进程进行调度的线程，这些线程对于操作系统是完全透明的，操作系统感知不到这些用户级线程的存在。其中比较出名的例子就是Java虚拟机JVM创建出来的Thread线程，通过设置Java虚拟机的参数可以改变为这些线程分配的栈的大小，其默认栈大小也是1MB。不过，对于一些比较简单的读请求，显然是用不到这么大的栈空间的，所以可以将默认栈大小设置的小一点。

至于Goroutine / 协程，其默认的栈大小则远远更小，只有2KB大小，使得一台服务器可以轻而易举的创建上万个协程而不会感到吃力，大大增加了并发度。同时，协程栈的大小是可以动态调整的，以适应不同请求的需求；早期的Go语言使用了分段栈，现版本的Go语言则采用了双倍扩容的策略。但Goroutine与用户级线程的区别并不仅仅在于栈的大小，而还在于切换代价，调度策略和抢占方式——否则修改JVM的参数即可，何必费那么大力气去发明一个新的语言呢。

网上有很多文章将Goroutine称之为“轻量级线程”。但如果轻量级线程定义为栈空间比较小的线程的话，那么这种说法将是不准确的，因为Goroutine的很多行为与线程有着很大的区别。

------

### 是谁在调度Goroutine?

Go语言是一个静态编译型语言，所以其不存在虚拟机这么一说。Goroutine的创建、调度与销毁都是由Go语言的运行时(runtime)来进行的，这有点像操作系统在的内核代码在管理用户进程一样，Go语言中的管理者就是runtime。

------

### GMP模型

#### G

G是Go语言中Goroutine的一个抽象，用结构体runtime.g来表示。这个结构体包含了很多字段，其中以下字段比较重要

```go
// 部分字段
type g struct {
	stack         stack 
	stackguard0   uintptr
	preempt       bool
	preemptStop   bool
	preemptShrink bool
	m             *m
	sched         gobuf
	atomicstatus  uint32
	goid          int64
}
```

下面来逐一介绍这些字段的含义：

- stack: 描述当前Goroutine的栈内存范围[low, high)
- stackguard0-preemptShirnk: 用于抢占式调度
- m: 当前goroutine占用的线程，可能为空
- atomicstatus: goroutine的状态
- sched: goroutine运行时的上下文
- goid: goroutine的ID，这个字段仅对runtime可见，即开发者无权查看。这是因为**Go团队认为如果开发者能够使用goid会使得某些goroutine变得更加特殊，从而影响公平性**

上面提到的sched的类型为`gobuf`，它也是一个结构体

```go
// 全部字段
type gobuf struct {
	sp   uintptr
	pc   uintptr
	g    guintptr
	ctxt unsafe.Pointer
	ret  sys.Uintreg
	lr   uintptr
	bp   uintptr 
}
```

可以看到Goroutine的上下文非常轻量，确切地说，在进行Goroutine的上下文切换时，仅需要很少的寄存器就可以确定一个Goroutine的状态，其中最重要的两个是`SP`和 `PC`。这与我们之前遇到的线程调度是完全不一样的，在线程调度时，首先会进入内核栈将当前线程的上下文全部压栈，这些上下文包含了几十个寄存器，包括通用寄存器/AVX/Floating Point/MMX等等...其代价相比Goroutine是非常巨大的，这也是为什么我们不能简单地将Goroutine称之为轻量级线程，因为它们有本质上的区别。



#### M

M则是内核级线程/操作系统线程的一个抽象，因为对于操作系统来说，它是感知不到Goroutine的存在的，在操作系统看来，只有一个接一个的线程在执行，所以我们需要做这层抽象去将G和M绑定起来，使得每一个操作系统线程都对应着一系列的Goroutine以供执行和调度

M的结构体:

```go
type m struct {
    g0      	  *g     // 带有调度栈的goroutine

    gsignal       *g         // 处理信号的goroutine
    tls           [6]uintptr // thread-local storage
    mstartfn      func()
    curg          *g       // 当前运行的goroutine
    caughtsig     guintptr 
    p             puintptr // 关联p和执行的go代码
    nextp         puintptr
    id            int32
    mallocing     int32 // 状态

    spinning      bool // m是否out of work
    blocked       bool // m是否被阻塞
    inwb          bool // m是否在执行写屏蔽

    printlock     int8
    incgo         bool // m在执行cgo吗
    fastrand      uint32
    ncgocall      uint64      // cgo调用的总数
    ncgo          int32       // 当前cgo调用的数目
    park          note
    alllink       *m // 用于链接allm
    schedlink     muintptr
    mcache        *mcache // 当前m的内存缓存
    lockedg       *g // 锁定g在当前m上执行，而不会切换到其他m
    createstack   [32]uintptr // thread创建的栈
}
```

其中g0是一个很特殊的Goroutine，它负责Goroutine的调度。此外，因为Goroutine是动态创建的，所以所有为Goroutine分配的栈空间实际上都是在虚拟地址空间的堆里，即堆空间里面有很多栈。当某些栈空间不足时，将会由runtime把它们复制并扩容至原大小的两倍。然而g0并不是在堆空间中的，严格来讲，它应该属于runtime，它的函数调用栈在栈空间中而非堆中

按照上面的描述，每个M应该至少维护一个G的队列才对，但在上述结构体中并未看到上述字段，这是因为Go在1.0之后的版本又对Goroutine模型多做了一层抽象



#### P

P(Processor)是这层介于G和M之间的新的一层抽象，它代表了处理去，或者当前M执行的上下文

因为每个线程都需要在一个CPU的核上运行，所以每个M在运行时，都需要绑定到一个P上面。P的个数为GOMAXPROCS，其默认值为CPU的核数，这样可以最大化地利用机器的多核性能

而M和P的关系则是多对一的，也就是说M的个数可以远远多于P的个数(最多有10000个M)。从M的角度来看，是每一个M都需要绑定到一个P上面，而从P的角度来看，则是每个P都需要寻找一个M来执行绑定在自己身上的Goroutine队列。所以上面问题的答案就很清楚了，这些Goroutine的执行队列与P结构体绑定而不是M

P有两种队列：本地队列和全局队列

- 本地队列：当前P的执行队列，本地队列是Lock-Free的，没有数据竞争问题，所以无需加锁处理，这样可以提升处理速度
- 全局队列：全局队列是为了保证多个P之间任务的平衡。所有M共享P的全局队列，为保证数据竞争问题，需要加锁处理

所以，在G与M之间多了这么一层P的抽象，一方面可以平衡各个M之间的任务负担：如果某一个M所属的P的执行队列为空，那么它就会去执行全局队列中的Goroutine，如果全局队列仍然为空，那么它就会去从别的P中“抢”执行任务，一次会抢走另一个P中的一半数量的任务。上述“抢”的特性于2012年被添加到Go语言中，称为**任务窃取调度器**

P的结构体:

```go
type p struct {
    lock mutex

    id          int32
    status      uint32 // 状态，可以为pidle/prunning/...
    link        puintptr
    schedtick   uint32     // 每调度一次加1
    syscalltick uint32     // 每一次系统调用加1
    sysmontick  sysmontick 
    m           muintptr   // 回链到关联的m
    mcache      *mcache
    racectx     uintptr

    goidcache    uint64 // goroutine的ID的缓存
    goidcacheend uint64

    // 可运行的goroutine的队列
    runqhead uint32
    runqtail uint32
    runq     [256]guintptr

    runnext guintptr // 下一个运行的g

    sudogcache []*sudog
    sudogbuf   [128]*sudog

    palloc persistentAlloc // per-P to avoid mutex

    pad [sys.CacheLineSize]byte
}
```

------

### 单线程调度器 0.x

0.x 版本调度器只包含表示 Goroutine 的 G 和表示线程的 M 两种结构，全局也只有一个线程，是用c语言实现的

```go
// 单线程调度器：find a G to run, run it, repeat
static void scheduler(void) {
    // G表示Goroutine && gp表示当前的Goroutine(G_present)，此时还没有初始化
	G* gp;
    // 获取调度器的全局锁
	lock(&sched);

    // gosave()函数保存栈寄存器和PC寄存器，但无需保存通用寄存器，因为仍在同一个线程内
	if(gosave(&m->sched)){
        // 在源码中有可能会通过其他函数jump到这里，没有执行上面的lock()，所以再执行一次
		lock(&sched);
        // gp指针指向当前线程的curg(current goroutine)
		gp = m->curg;
        // 修改当前goroutine的状态
		switch(gp->status){
		case Grunnable:
		case Grunning:
			gp->status = Grunnable;
            // 挂起
			gput(gp);
			break;
		...
		}
		notewakeup(&gp->stopped);
	}
	
    // 获取下一个需要运行的goroutine并解锁调度器(next_g_and_unlock)
	gp = nextgandunlock();
	noteclear(&gp->stopped);
    // 修改当前goroutine状态
	gp->status = Grunning;
    // 修改当前线程上要执行的goroutine
	m->curg = gp;
	g = gp;
    // 调用gogo函数运行最新的goroutine
	gogo(&gp->sched);
}
```

只包含表示Goroutine的G和表示线程的M两种结构

也就是说在0.x的版本下，每个go程序是只能利用单个线程的，虽然完成了goroutine的基本实现，但可以认为是没用，因为完全无法利用CPU的多核性能，在单个线程内不停的做调度有什么意义呢？毫无疑问这对于并发是没有帮助的。这个版本的意义在于实现一个基本能用的调度器模型，为接下来的多线程调度器做准备，但***完全没有实用价值***

------

### 多线程调度器 1.0

```c
static void schedule(G *gp) {
	schedlock();
	if(gp != nil) {
		gp->m = nil;
		uint32 v = runtime·xadd(&runtime·sched.atomic, -1<<mcpuShift);
		if(atomic_mcpu(v) > GOMAXPROCS)
			runtime·throw("negative mcpu in scheduler");

		switch(gp->status){
		case Grunning:
			gp->status = Grunnable;
			gput(gp);
			break;
		case ...:
		}
	} else {
		...
	}
	gp = nextgandunlock();
	gp->status = Grunning;
	m->curg = gp;
	gp->m = m;
	runtime·gogo(&gp->sched, 0);
}
```

只是单线程调度器的一个多线程版本，仍然采用了G-M模型，尚未引入P。这样的做的不足是**所有的M上的运行队列都是全局的**，所以每次选择一个G执行的时候都需要对其加全局锁，这是一个不小的开销

------

### 任务窃取调度器

这个版本引入了P，上文已经提及

具体来讲，处理器持有一个由可运行的Goroutine组成的唤醒的运行队列runq，还反向持有一个线程。调度器在调度时会从处理器的队列中选择队列头的Goroutine放到线程M上执行

这个版本的调度器伴随了Go语言很久。其具有以下特性：

- 此时的Goroutine的严格的coroutine，其调度是严格基于协作式抢占的，即仅当某一个Goroutine涉及到锁或IO等阻塞操作时才会触发Goroutine的yield

------

### 基于协作的抢占式调度器

这是Go语言第一个能够使某个Goroutine的执行权被“抢占”的调度器

无论是在操作系统还是JVM中，线程的调度都是可抢占的，抢占的最基本的实现就是通过时钟中断来强行进入内核态从而执行调度逻辑来选择下一个想要执行的线程。这样做的好处是避免了某一个线程一直占用CPU时间不放的可能。但为什么Go语言直到这个版本才添加了抢占式调度的特性呢？我认为这是因为相比于操作系统，Go程序中的Goroutine是可控的，如果某一个Goroutine一直占用着CPU不放，那么说明这是一个CPU密集型的Goroutine，此时如果强行把它调度走，虽然能够使其他Goroutine获得执行权，但程序总体的执行时间却并不会因此而减少，因为当前的Goroutine**并未被阻塞**——所以让它一直享有CPU的使用权似乎也无关紧要

但这么做却与Go语言的设计原则相违背，Go的设计原则一直在强调**所有的Goroutine都是平等的**，但这种情况显然引入了不公平，所以在1.13版本中Go社区引入了基于协作的抢占式调度，其基于分段栈基址。具体原理如下：

- 在Go语言进行GC(垃圾收集)时，是需要STW(Stop the world)的
- 在STW期间sysmon(系统监视器，这是一个单独的线程，由runtime在启动时建立)会检查所有正在执行的Goroutine的运行时间，如果其运行时间已经超过了10ms，则会为其做一个标记，礼貌地告诉它该要让出执行权了
- STW结束后，当这个被标记的Goroutine在call一个函数时，会用到扩容站的部分逻辑，检测到抢占标记之后，让出CPU

所以，这个Goroutine让出CPU的条件是其主动call一个函数，只有在这时它才能检测到自己被标记了

但这样仍然不能解决某些特定场景下的问题：如果某个Goroutine陷入了死循环会怎样？

比如一个Goroutine在执行`for {i++}`，那么它将永远不会进行函数调用，也就没有机会去检查自己是否被标记了

所以，这个版本的调度器仍然有缺陷

------

### 基于系统信号的抢占式调度器

在go1.14版本中引入了基于系统信号的异步抢占调度器

在1.14版本之前抢占是由sysmon来检测的，在1.14版本中也一样

如果sysmon线程发现了某个Goroutine的执行时间超过了10ms，则会对其发送SIGURG信号，线程M收到了这个信号之后会通过其绑定的P找到当前正在执行的G，然后进行解绑操作，封存当前Goroutine，执行调度逻辑

------

### 上下文切换代价

上文提到了Goroutine相比线程的一大优势就是切换代价，无论是用户态的线程还是内核态的线程，在切换时都是需要保存上下文的，这些上下文包含了通用寄存器在内的几十个寄存器，代价很大

相比之下，因为Goroutine**只会在特定的时刻执行调度逻辑**，比如：

- 在call一个函数时
- 执行了阻塞的操作时/执行系统调用时
- 主动挂起
- 系统监控

在以上四种情况中，所有的调度都不是**异步**的，即不会由某个Goroutine会突然在不知情的情况下被剥夺CPU的执行权，在这些调度点Goroutine仅需保存当前正在被使用的寄存器即可。其中最重要的就是SP和PC，这两个寄存器指定了Goroutine的运行栈和指令地址

------

### Goroutine与锁

#### Sync.Mutex

Mutex是Go语言标准库中提供的互斥锁，同很多其他编程语言一样，其提供了并发某一对象的控制。不同的语言可能有不同的实现mutex的方式，但mutex的根本思想是一旦某个线程/协程获得了对于某个对象的访问权并进行加锁，其他的线程/协程则必须等待，直到这个拿到了锁的线程/协程将其释放。为了叙述方便，一下将所有的线程/协程同一表示为Go语言环境下的Goroutine，即协程.

Go语言对于Mutex的实现总共有过三个改动比较大的版本，这三个版本的共性是每个版本的Mutex上都有一个等待队列，队列上的Goroutine遵守FIFO的原则对锁进行访问，当某一个Goroutine释放了锁并修改了信号量后，其会唤醒处于等待队列头部的Goroutine.

- 早期的第一个Mutex版本commit于大概2008年，这时的go语言设计原则是能用就行，暂时不考虑性能相关的问题，所以Mutex的设计也比较简单。当某一个Goroutine拿到了某个对象的锁之后，当其他Goroutine发现这个对象的锁已经被占有的话，这些后来的Goroutine就会**直接将自己阻塞并休眠并放在等待队列的队尾**

- 第二个版本的Mutex则在此基础上做了自旋的优化。其设计原则基于以下两个事实：

  1. 在绝大多数情况下，某个Goroutine获得到一个锁以后，只会持有这个锁相当短的一段时间，因为大多数Goroutine做的都是读(内存)的操作，速度很快
  2. 现在的机器都是多核的，我们都知道在单核的机器上使用自旋锁是毫无意义的，而只会白白浪费CPU的使用率。在多核的机器上可能某一个正在等待锁的Goroutine和另外一个持有并即将释放锁的Goroutine恰好处于两个不同的CPU核上，这就使得自旋等待有了意义

  在这样“一定的自旋可能会增加效率”的思想下，第二个版本的Mutex的行为变成了：在某个Goroutine已经持有锁的情况下，后续的Goroutine会执行**一些逻辑**来判断是否要自旋，这些逻辑主要是为了判断在当下的情况下执行自旋是否有意义，比如如果两个竞争的Goroutine处于同一个CPU核心下那么自旋就无意义。如果逻辑判断通过，那么Goroutine会自旋一段时间，如果这段时间之内锁被释放，那么后来的这个Goroutine就获得了执行的机会，否则将自己休眠并放在等待队列的队尾

- 上面的第二个版本看似相比于第一个版本的Mutex已经做了很多将多核性能利用起来的优化，但在2015年的时候，有人在Go语言社区提出了一个issue并描述了在特定情况下第二个版本的可能缺陷。首先，值得注意的一点是**当某一个Goroutine释放了锁并唤醒等待队列首部的Gotoutine后，这个Goroutine并不会立即休眠，其可能还持有一段CPU的使用时间**。这样，当这个被唤醒的Goroutine醒来时，很可能发现在其醒来之前，这个锁早就被其他CPU核心上的Goroutine或者是释放锁的Goroutine抢走了，这段时间差使得刚刚被唤醒的Goroutine**往往拿不到锁**。当其发现锁无法被拿到，它就会在一段自旋的时间后再次将自己休眠，并将自己重新放倒等待队列的队尾。形象的说：某一个Goroutine在排了半天队以后未能获得服务，只好再次排队。

  Go语言开发者们为了解决这个问题，在第三个版本的Mutex中在Mutex中添加了一个新的状态`Starving`。当一个Goroutine总共等待了超过1ms的时间后，其就会将Mutex的状态由`Normal`变为`Starving`以表示自己的不满，这时后来的Goroutine看到这个锁虽然是处于解锁状态的，但因为其状态是`Starving`，表示某一个Goroutine已经很久没有获得过这个锁了，就会主动将自己放在等待队列的队尾。这样就避免了某一个被唤醒的进程发现锁已经被抢走了。

  同时，这个版本的Mutex也将触发自旋的逻辑修改为同时满足如下条件：

  1. Mutex只有在`Normal`模式下才会触发自旋
  2. 运行在多CPU的机器上
  3. 当前 Goroutine 为了获取该锁进入自旋的次数小于四次
  4. 当前机器上至少存在一个正在运行的处理器 P 并且处理的运行队列为空

如此经过了三个版本的迭代，便有了Go语言当下版本的Mutex实现



#### Sync.RWMutex

如今大部分数据库产品都支持读写锁RWMutex，这是基于大部分对于服务的访问都是读操作而只有少部分是写操作这样一个事实。然而读操作之间是不会影响的，只有读和写、写和写操作之间是冲突的需要加锁，所以有了读写锁：不同的读线程可以同时读某一内存，而写线程则对于这块内存的访问是独占的。这与操作系统中的copy on write操作的设计理念很像。

Go语言中也同样支持RWMutex，不过其基于传统的RWMutex做了一些改进以尽量减少某个Goroutine饿死的情况

Go语言中RWMutex的结构体:

```go
type RWMutex struct {
	w           Mutex  // 复用互斥锁提供的功能
	writerSem   uint32 // 用于等待读
	readerSem   uint32 // 用于等待写
	readerCount int32  // 当前正在执行读操作的Goroutine的数量
	readerWait  int32  // 当写操作被阻塞时等待的读操作Goroutine的数量
}
```

可以看到读写锁的实现是基于互斥锁的

- 获取写锁：
  1. 首先，这个Goroutine会将RWMutex.Mutex锁住，以**阻塞后续的写操作**
  2. 将 `readerCount` 减少 `rwmutexMaxReaders` 个数以阻塞后续的读操作
  3. 如果当前仍有读Goroutine，则该线程会将自己休眠并等待所有的读Goroutine执行完毕并释放信号量
- 释放写锁：
  1. 首先，将`readerCount`变为非负数，其效果为释放读锁
  2. 通过一个for循环唤醒所有阻塞的读Goroutine
  3. 释放写锁
- 获取读锁：
  1. 首先将`readerCount`++，如果这个方法返回了一个负数，则说明有其它的Goroutine已经获得了写锁，则直接将自己放入阻塞队列的尾部并休眠
  2. 如果返回的是一个非负数，那么说明当前没有Goroutine在竞争或所有的Goroutine都只是在读，则成功获取读锁
- 释放读锁：
  1. 将`readerCount`--，如果返回一个非负数，读锁直接解锁成功
  2. 如果返回的一个负数，会减少获取锁的写操作等待的读操作数 `readerWait` 并在所有读操作都被释放之后触发写操作的信号量 `writerSem`，该信号量被触发时，调度器就会唤醒尝试获取写锁的 Goroutine

可以看到，Go在实现RWMutex的时候综合考虑了读Goroutine和写Goroutine的公平性，避免了其中一方的饿死可能。这与操作系统课上讲的读者写者问题的解决方案有着同样的效果，也是一种不错的解决方案

------

### 总结

在设计操作系统的进程调度系统时，我们需要考虑进程/线程的优先级，这是因为有些应用是前台的，需要与用户实时交互，而有些应用是后台的，并不需要优先调度。基于这样的思想，在操作系统的实现中通常会为所有待调度的执行单位分配一个优先级来表示它们彼此之间哪个更加重要

相比之下Go语言则自始至终都在强调Goroutine之间的公平性，Go语言的设计者认为所有的Goroutine都应该被平等对待。这种设计原则使得它的调度器实现相对于操作系统简单很多，只是一个先进先出的队列，而不会从队列中按照优先级选择想要调度的Goroutine，甚至连Goroutine的goid都对程序员屏蔽了，以避免某些Goroutine受到不公平的对待

这两种调度方式显然没有孰优孰劣这么一说，因为它们想要达成的目的不同。对于Go程序来讲，我们只是想要它拥有最高的并发度（因为Go语言是用来处理网络请求而非开发桌面应用的，所以不需要实时交互）。基于此，早期的调度方案甚至不允许抢占，后来的版本中因为一些极端的特殊情况才不得不将这个特性移除了，但从这些历史版本我们还是能够总结出Goroutine的设计哲学：

1. 一切为并发服务
2. 公平，公平，还是公平